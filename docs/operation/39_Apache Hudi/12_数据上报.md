# 数据上报

## 本内容你将获得

- 如何在数据目录平台上报数据
- 如何将kafka数据入湖

## 数据模板上传

### 1、编写excel导入模板

a)、新建excel文件

b)、填写字段名称、字段英文名称、字段含义、是否必填、数据类型、枚举值、数据样例。如下图所示

<img :src="$withBase('/operation/data_hudi_01.png')">

第1行到第7行为导入模板信息。用户下载导入模板后，从第8行开始填写上报信息，其中第1列为数据序号。填写完后，为数据加上框线。

C)、保存为.xlsx文件

### 2、上传数据目录平台

a)、登录数据目录平台，点击上传-->上传文件，选择导入模板上传到平台

<img :src="$withBase('/operation/data_hudi_02.png')">

<img :src="$withBase('/operation/data_hudi_03.png')">

b)、选择已上传的导入模板记录，点击右键，设置入库配置信息

<img :src="$withBase('/operation/data_hudi_04.png')">

c)、在弹出窗中,根据提示填写导入模板相关的入库信息

<img :src="$withBase('/operation/data_hudi_05.png')">



其中数据模型：选择是

项目名称：数据来源的项目名称

库名：数据来源的数据库名称

表名：数据来源的数据表名称

kafka主题：接收上报数据的kafka主题

kafka表：连接kafka表的建表语句

hudi表：连接hudi表的建表语句

入库信息：从kafka插入hudi的入库语句

填写完信息并确认后，点击确定

## 编写入库代码及部署

1、在alinesno-data-hudi工程中，新建ElectricityBill类

```java
package com.alinesno.cloud.data.hudi.kafka;

import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.runtime.state.filesystem.FsStateBackend;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.environment.CheckpointConfig;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

import static org.apache.flink.table.api.Expressions.$;


public class ElectricityBill {

    public   void slinkElectricityBill() {

        //1.获取表的执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //2.1设置检查点信息
        //由于增量将数据写入到hudi表，所以需要启动Flink Checkpoint检查点
        env.setParallelism(2);           //并行度

        //每 5 * 1000 ms 开始执行检查点
        env.enableCheckpointing(5 * 1000);

        CheckpointConfig checkpointconf = env.getCheckpointConfig();

        // make sure 3000 ms of progress happen between checkpoints
        checkpointconf.setMinPauseBetweenCheckpoints(3000l);

        // checkpoints have to complete within one minute, or are discarded
        checkpointconf.setCheckpointTimeout(60000l);

        //2.2 指定 CK 的一致性语义
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        //2.3 设置任务关闭的时候保留最后一次 CK 数据
        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
        //2.4 指定从 CK 自动重启策略
        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(30, 2000L));
        //2.5 设置检查点的存储文件
        env.setStateBackend(new FsStateBackend("hdfs://172.17.49.195:9000/user/flink/hudi/ck/sink_electricity_bill"));


        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                .inStreamingMode()//设置流式模式
                .build();
        //2.6 设置访问 HDFS 的用户名
        System.setProperty("HADOOP_USER_NAME", "root");
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);


        //3.创建输入表，TODO:从kafka消费数据
        tableEnv.executeSql(
                "create table electricity_bill_kafka_source(\n" +
                        "    billing_month STRING,\n" +
                        "    provinces     STRING,\n" +
                        "    local_market  STRING,\n" +
                        "    county        STRING,\n" +
                        "    meter_number  STRING,\n" +
                        "    pay_amount    double \n" +
                        ")\n" +
                        "WITH(\n" +
                        "    'connector' = 'kafka',\n" +
                        "    'topic'='electricity_bill',\n" +
                        "    'properties.bootstrap.servers' = '172.17.49.195:9092',\n" +
                        "    'properties.group.id' = 'testgroup1',\n" +
                        "    'scan.startup.mode' = 'earliest-offset',\n" +
                        "    'format' = 'json'                      ,\n" +
                        "    'json.fail-on-missing-field' = 'false' ,\n" +
                        "    'json.ignore-parse-errors' = 'true'     \n" +
                        ")\n"
        );

 

        //4.创建输出表，TODO:关联到hudi表，指定hudi表名称，存储路径，字段名称等信息
        tableEnv.executeSql(
                "create table electricity_bill_hudi_sink(\n" +
                        "    billing_month STRING ,\n" +
                        "    provinces     STRING ,\n" +
                        "    local_market  STRING ,\n" +
                        "    county        STRING ,\n" +
                        "    meter_number  STRING PRIMARY KEY NOT ENFORCED ,\n" +
                        "    pay_amount    double ,\n" +
                        "    `part` VARCHAR(20) \n" +
                        ")\n" +
                        "PARTITIONED BY (`part`)\n" +
                        "WITH(\n" +
                        "    'connector' = 'hudi',\n" +
                        "    'path'='hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi',\n" +
                        "    'table.type' = 'COPY_ON_WRITE',\n" +
                        "    'write.operation' = 'upsert',\n" +
                        "    'write.tasks' = '1', \n" +
                        "    'hive_sync.enable'='true', -- required，开启hive同步功能\n" +
                        "    'hive_sync.mode' = 'hms', -- required, 将hive sync mode设置为hms, 默认jdbc\n" +
                        "    'hive_sync.metastore.uris' = 'thrift://172.17.49.195:9083', -- metastore的端口\n" +
                        "    'hive_sync.db' = 'test', -- hive新建数据库名\n" +
                        "    'hive_sync.table' = 'flink_electricity_bill_hudi', -- hive新建表名\n" +
                        "    'hive_sync.support_timestamp' = 'true' -- 兼容hive timestamp类型\n" +
                        ")\n"
        );

        //5.通过子查询的方式，将数据写入输出表
        tableEnv.executeSql(
                "insert into electricity_bill_hudi_sink " +
                        " select billing_month, provinces, local_market, county, meter_number, pay_amount, billing_month from electricity_bill_kafka_source "
        );


    }
}


```

2、在主类main中，调用ElectricityBill类

```java
package com.alinesno.cloud.data.hudi.kafka;

public class kafkaMain {

    public static void main(String[] args) {

        //从kafka接收网盘电费账单模板的数据，入库到hudi表
        ElectricityBill electricityBill = new ElectricityBill();
        electricityBill.slinkElectricityBill();

    }
}
```

3、编译alinesno-data-hudi工程

<img :src="$withBase('/operation/data_hudi_10.png')">

将jar包部署到flink管理界面进行测试

<img :src="$withBase('/operation/data_hudi_11.png')">

4、登录flink管理界面 http://alinesno-flink.history.ui.lbxinhu.linesno.com

<img :src="$withBase('/operation/data_hudi_12.png')">

a)、上传jar包

在提交新任务界面，上传jar包

<img :src="$withBase('/operation/data_hudi_13.png')">

<img :src="$withBase('/operation/data_hudi_14.png')">

<img :src="$withBase('/operation/data_hudi_15.png')">

点击jar包记录，设置jar包主类及并行度后，点击Submit进行调度测试

<img :src="$withBase('/operation/data_hudi_16.png')">

b)、查看运行信息

<img :src="$withBase('/operation/data_hudi_17.png')">

c)、查看运行结果

查看映射的hive表数据

```shell
hive> use test;
OK
Time taken: 0.029 seconds
hive> show tables;
OK
flink_electricity_bill_hudi
Time taken: 0.032 seconds, Fetched: 1 row(s)
hive> select *from flink_electricity_bill_hudi ;
OK
20220920095216264       20220920095216264_0_0   500300521431    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500300521431    599.35  202201
20220920095216264       20220920095216264_0_1   5639183901      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5639183901      760.34  202201
20220920095216264       20220920095216264_0_2   0201003677651   202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  0201003677651   590.87  202201
20220920095216264       20220920095216264_0_3   500048937031    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500048937031    28681.0 202201
20220920095216264       20220920095216264_0_4   5639078671      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5639078671      422.62  202201
20220920095216264       20220920095216264_0_5   5901128771      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5901128771      164.14  202201
20220920095216264       20220920095216264_0_6   500048182501    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500048182501    598.71  202201
20220920095216264       20220920095216264_0_7   219530951       202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  219530951       495.16  202201
20220920095216264       20220920095216264_0_8   500421725501    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500421725501    412.56  202201
20220920095216264       20220920095216264_0_9   5639212151      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5639212151      707.51  202201
20220920095216264       20220920095216264_0_0   500067512491    202201  949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500067512491    1166.61 202201
20220920095216264       20220920095216264_0_1   04121SF000000019005694161       202201  949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  04121SF000000019005694161       1021.96 202201
20220920095216264       20220920095216264_0_2   500226762451    202201  949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500226762451    1250.37 202201
Time taken: 1.245 seconds, Fetched: 13 row(s)
hive> 
```

查看对应的hdfs数据

```
[root@hadoopmaster ~]# hadoop fs -ls hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi
Found 2 items
drwxr-xr-x   - root supergroup          0 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/.hoodie
drwxr-xr-x   - root supergroup          0 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201
[root@hadoopmaster ~]# hadoop fs -ls hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201
Found 3 items
-rw-r--r--   3 root supergroup         96 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201/.hoodie_partition_metadata
-rw-r--r--   3 root supergroup     436476 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201/2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet
-rw-r--r--   3 root supergroup     436415 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201/949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet
[root@hadoopmaster ~]# 
```



## 上报数据

a)、登录数据目录平台，点击上传-->上传报表表格，进入报表表格导入弹出窗界面。

<img :src="$withBase('/operation/data_hudi_06.png')"> 

b)、选择导入模板

<img :src="$withBase('/operation/data_hudi_07.png')"> 



<img :src="$withBase('/operation/data_hudi_08.png')"> 



c)、下载导入模板

<img :src="$withBase('/operation/data_hudi_09.png')"> 

d)、上传数据文件

按照导入模板填写好数据后，点击上传报表文件，选择填写好的数据文件。

<img :src="$withBase('/operation/data_hudi_09.png')"> 

e)、上报数据

选择要上报的记录，点击提取，将数据文件从云存储下载到本地；点击上报，系统读取数据文件中的数据并发往kafka消息总线。

<img :src="$withBase('/operation/data_hudi_18.png')"> 



 





