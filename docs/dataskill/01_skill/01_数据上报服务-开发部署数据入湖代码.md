# 部署入库服务

## 概述 

开发、部署接收导入模板数据，将数据写入数据仓库的服务

## 本内容你将获得

- 如何开发及部署导入模板的入库服务

### 开发代码

 在alinesno-data-hudi工程中，根据业务需求新建入库类。参考ElectricityBill类

```java
package com.alinesno.cloud.data.hudi.kafka;

import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.runtime.state.filesystem.FsStateBackend;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.environment.CheckpointConfig;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

import static org.apache.flink.table.api.Expressions.$;


public class ElectricityBill {

    public   void slinkElectricityBill() {

        //1.获取表的执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //2.1设置检查点信息
        //由于增量将数据写入到hudi表，所以需要启动Flink Checkpoint检查点
        env.setParallelism(2);           //并行度

        //每 5 * 1000 ms 开始执行检查点
        env.enableCheckpointing(5 * 1000);

        CheckpointConfig checkpointconf = env.getCheckpointConfig();

        // make sure 3000 ms of progress happen between checkpoints
        checkpointconf.setMinPauseBetweenCheckpoints(3000l);

        // checkpoints have to complete within one minute, or are discarded
        checkpointconf.setCheckpointTimeout(60000l);

        //2.2 指定 CK 的一致性语义
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        //2.3 设置任务关闭的时候保留最后一次 CK 数据
        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
        //2.4 指定从 CK 自动重启策略
        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(30, 2000L));
        //2.5 设置检查点的存储文件
        env.setStateBackend(new FsStateBackend("hdfs://172.17.49.195:9000/user/flink/hudi/ck/sink_electricity_bill"));


        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                .inStreamingMode()//设置流式模式
                .build();
        //2.6 设置访问 HDFS 的用户名
        System.setProperty("HADOOP_USER_NAME", "root");
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env, settings);


        //3.创建输入表，TODO:从kafka消费数据
        tableEnv.executeSql(
                "create table electricity_bill_kafka_source(\n" +
                        "    billing_month STRING,\n" +
                        "    provinces     STRING,\n" +
                        "    local_market  STRING,\n" +
                        "    county        STRING,\n" +
                        "    meter_number  STRING,\n" +
                        "    pay_amount    double \n" +
                        ")\n" +
                        "WITH(\n" +
                        "    'connector' = 'kafka',\n" +
                        "    'topic'='electricity_bill',\n" +
                        "    'properties.bootstrap.servers' = '172.17.49.195:9092',\n" +
                        "    'properties.group.id' = 'testgroup1',\n" +
                        "    'scan.startup.mode' = 'earliest-offset',\n" +
                        "    'format' = 'json'                      ,\n" +
                        "    'json.fail-on-missing-field' = 'false' ,\n" +
                        "    'json.ignore-parse-errors' = 'true'     \n" +
                        ")\n"
        );

 

        //4.创建输出表，TODO:关联到hudi表，指定hudi表名称，存储路径，字段名称等信息
        tableEnv.executeSql(
                "create table electricity_bill_hudi_sink(\n" +
                        "    billing_month STRING ,\n" +
                        "    provinces     STRING ,\n" +
                        "    local_market  STRING ,\n" +
                        "    county        STRING ,\n" +
                        "    meter_number  STRING PRIMARY KEY NOT ENFORCED ,\n" +
                        "    pay_amount    double ,\n" +
                        "    `part` VARCHAR(20) \n" +
                        ")\n" +
                        "PARTITIONED BY (`part`)\n" +
                        "WITH(\n" +
                        "    'connector' = 'hudi',\n" +
                        "    'path'='hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi',\n" +
                        "    'table.type' = 'COPY_ON_WRITE',\n" +
                        "    'write.operation' = 'upsert',\n" +
                        "    'write.tasks' = '1', \n" +
                        "    'hive_sync.enable'='true', -- required，开启hive同步功能\n" +
                        "    'hive_sync.mode' = 'hms', -- required, 将hive sync mode设置为hms, 默认jdbc\n" +
                        "    'hive_sync.metastore.uris' = 'thrift://172.17.49.195:9083', -- metastore的端口\n" +
                        "    'hive_sync.db' = 'test', -- hive新建数据库名\n" +
                        "    'hive_sync.table' = 'flink_electricity_bill_hudi', -- hive新建表名\n" +
                        "    'hive_sync.support_timestamp' = 'true' -- 兼容hive timestamp类型\n" +
                        ")\n"
        );

        //5.通过子查询的方式，将数据写入输出表
        tableEnv.executeSql(
                "insert into electricity_bill_hudi_sink " +
                        " select billing_month, provinces, local_market, county, meter_number, pay_amount, billing_month from electricity_bill_kafka_source "
        );


    }
}


```

代码说明：

- 设置检查点的存储文件
- 创建输入表、输出表。按照业务需求加工输入表数据后写入输出表

在主类中，调用类。如ElectricityBill

```java
package com.alinesno.cloud.data.hudi.kafka;

public class kafkaMain {

    public static void main(String[] args) {

        //kafka接收数据上报服务数据，入库到hudi表
        ElectricityBill electricityBill = new ElectricityBill();
        electricityBill.slinkElectricityBill();

    }
}
```

### 编译代码

编译alinesno-data-hudi工程

<img :src="$withBase('/operation/data_hudi_10.png')">

### 部署服务

将jar包上传到flink的Submit New Job界面，设置运行主类后提交。在Jobs-Running Jobs界面，检查到任务正常运行，即可等待数据上报。

a)、查看部署信息

<img :src="$withBase('/operation/data_hudi_17.png')">

b)、查看部署结果

查看hudi表对应的hdfs数据

```shell
[root@hadoopmaster ~]# hadoop fs -ls hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi
Found 2 items
drwxr-xr-x   - root supergroup          0 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/.hoodie
drwxr-xr-x   - root supergroup          0 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201
[root@hadoopmaster ~]# hadoop fs -ls hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201
Found 3 items
-rw-r--r--   3 root supergroup         96 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201/.hoodie_partition_metadata
-rw-r--r--   3 root supergroup     436476 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201/2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet
-rw-r--r--   3 root supergroup     436415 2022-09-20 09:52 hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201/949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet
[root@hadoopmaster ~]# 
```

查看映射的hive表数据

```shell
hive> use test;
OK
Time taken: 0.029 seconds
hive> show tables;
OK
flink_electricity_bill_hudi
Time taken: 0.032 seconds, Fetched: 1 row(s)
hive> select *from flink_electricity_bill_hudi ;
OK
20220920095216264       20220920095216264_0_0   500300521431    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500300521431    599.35  202201
20220920095216264       20220920095216264_0_1   5639183901      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5639183901      760.34  202201
20220920095216264       20220920095216264_0_2   0201003677651   202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  0201003677651   590.87  202201
20220920095216264       20220920095216264_0_3   500048937031    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500048937031    28681.0 202201
20220920095216264       20220920095216264_0_4   5639078671      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5639078671      422.62  202201
20220920095216264       20220920095216264_0_5   5901128771      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5901128771      164.14  202201
20220920095216264       20220920095216264_0_6   500048182501    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500048182501    598.71  202201
20220920095216264       20220920095216264_0_7   219530951       202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  219530951       495.16  202201
20220920095216264       20220920095216264_0_8   500421725501    202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500421725501    412.56  202201
20220920095216264       20220920095216264_0_9   5639212151      202201  2304c08d-af1f-4356-8fb3-bbf739c30aad_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  5639212151      707.51  202201
20220920095216264       20220920095216264_0_0   500067512491    202201  949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500067512491    1166.61 202201
20220920095216264       20220920095216264_0_1   04121SF000000019005694161       202201  949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  04121SF000000019005694161       1021.96 202201
20220920095216264       20220920095216264_0_2   500226762451    202201  949f774e-cce5-4c53-8436-0f39582f3041_0-1-0_20220920095216264.parquet    202201  广西    南宁    良庆区  500226762451    1250.37 202201
Time taken: 1.245 seconds, Fetched: 13 row(s)
hive> 
```

配置说明

- 删除测试数据  hdfs dfs -rm -r  hdfs://172.17.49.195:9000/user/flink/hudi/flink_electricity_bill_hudi/202201 
