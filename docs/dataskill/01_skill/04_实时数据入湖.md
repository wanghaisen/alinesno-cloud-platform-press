# 实时数据入湖

## 本内容你将获得

- 实时捕获数据变化入湖

## 前提条件

MySQL 开启 binlog 日志

## 实时捕获数据变化入湖步骤

- 开发、编译代码
- 部署jar包

### 开发、编译代码

 在alinesno-data-hudi工程中，根据业务需求新建捕获数据类。参考user_file_list类

```java
package com.alinesno.cloud.data.hudi.mysql;

import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.runtime.state.filesystem.FsStateBackend;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.environment.CheckpointConfig;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

public class user_file_list {
    public static void main(String[] args) {

        //1.获取执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //2.1设置检查点信息
        //由于增量将数据写入到hudi表，所以需要启动Flink Checkpoint检查点
        env.setParallelism(2);           //并行度

        env.enableCheckpointing(5000L);  //5000毫秒即5秒一个检查点
        CheckpointConfig checkpointconf = env.getCheckpointConfig();
        checkpointconf.setMinPauseBetweenCheckpoints(30000l);
        checkpointconf.setCheckpointTimeout(10000000l);

        //2.2 指定 CK 的一致性语义
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        //2.3 设置任务关闭的时候保留最后一次 CK 数据
        env.getCheckpointConfig().enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
        //2.4 指定从 CK 自动重启策略
        env.setRestartStrategy(RestartStrategies.fixedDelayRestart(30, 2000L));
        //2.5 设置状态后端
        env.setStateBackend(new FsStateBackend("hdfs://172.17.49.195:9000/user/flink/hudi/ck/population/sink_user_file_list"));

        EnvironmentSettings settings = EnvironmentSettings
                .newInstance()
                .inStreamingMode() //设置流失模式
                .build();

        //2.6 设置访问 HDFS 的用户名
        System.setProperty("HADOOP_USER_NAME", "root");
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env,settings);

        //2.DDL方式建表
        /*
        在flink1.13版本支持根据mysql主键多并发读取数据功能，如果mysql没有设置主键，with里面要加'scan.incremental.snapshot.enabled' = 'false'否则会报错:
         */

        //个人照片库需要从userfile、file、user等多个表关联取出数据
        //userfile //文件id  //文件名称   //上报平台的文件路径  //文件格式 //用户ID //上传时间
        String source_userfile = "CREATE TABLE source_userfile ( " +
                " userFileId varchar(20) PRIMARY KEY NOT ENFORCED, " +
                " fileName   varchar(100), " +
                " filePath   varchar(500), " +
                " extendName varchar(100), " +
                " fileId     varchar(20), "  +
                " userId     bigint, "       +
                " uploadTime varchar(25) "   +
                ") WITH ( " +
                " 'connector' = 'mysql-cdc', " +
                " 'hostname' = '172.17.49.192', " +
                " 'port' = '3306', " +
                " 'username' = 'root', " +
                " 'password' = 'Xinhu#*UI!@?_ty*48YHJ$$#]y^n', " +
                " 'server-time-zone'= 'Asia/Shanghai', " +
                " 'scan.startup.mode'='initial'," +
                " 'database-name' = 'dev_alinesno_cloud_data_report', " +
                " 'table-name' = 'userfile' " +
                ")";
                
        //file //文件id  //创建时间  //创建用户id  //文件大小  //文件状态  //minIO中的存储位置
        String source_file = "CREATE TABLE source_file ( " +
                " fileId       varchar(20) PRIMARY KEY NOT ENFORCED, " +
                " createTime   varchar(25),                          " +
                " createUserId bigint,                               " +
                " fileSize     bigint,                               " +
                " fileStatus   int,                                  " +
                " fileUrl      varchar(500)                          " +
                ") WITH ( " +
                " 'connector' = 'mysql-cdc', " +
                " 'hostname' = '172.17.49.192', " +
                " 'port' = '3306', " +
                " 'username' = 'root', " +
                " 'password' = 'Xinhu#*UI!@?_ty*48YHJ$$#]y^n', " +
                " 'server-time-zone'= 'Asia/Shanghai', " +
                " 'scan.startup.mode'='initial'," +
                " 'database-name' = 'dev_alinesno_cloud_data_report', " +
                " 'table-name' = 'file' " +
                ")";

        //user //用户id   //用户名 //手机号 用于取用户清单 
        String source_user = "CREATE TABLE source_user ( " +
                " userId       bigint PRIMARY KEY NOT ENFORCED, " +
                " username     varchar(30),                     " +
                " telephone    varchar(15)                      " +
                ") WITH ( " +
                " 'connector' = 'mysql-cdc', " +
                " 'hostname' = '172.17.49.192', " +
                " 'port' = '3306', " +
                " 'username' = 'root', " +
                " 'password' = 'Xinhu#*UI!@?_ty*48YHJ$$#]y^n', " +
                " 'server-time-zone'= 'Asia/Shanghai', " +
                " 'scan.startup.mode'='initial'," +
                " 'database-name' = 'dev_alinesno_cloud_data_report', " +
                " 'table-name' = 'user' " +
                ")";

        //fileclassification //用户id //文件扩展名 //文件类型id   
        String source_fileclassification = "CREATE TABLE source_fileclassification ( " +
                " fileClassificationId   bigint PRIMARY KEY NOT ENFORCED, " +
                " fileExtendName         varchar(25),                     " +
                " fileTypeId             bigint                           " +
                ") WITH ( " +
                " 'connector' = 'mysql-cdc', " +
                " 'hostname' = '172.17.49.192', " +
                " 'port' = '3306', " +
                " 'username' = 'root', " +
                " 'password' = 'Xinhu#*UI!@?_ty*48YHJ$$#]y^n', " +
                " 'server-time-zone'= 'Asia/Shanghai', " +
                " 'scan.startup.mode'='initial'," +
                " 'database-name' = 'dev_alinesno_cloud_data_report', " +
                " 'table-name' = 'fileclassification' " +
                ")";


        //个人照片库 latest-offset  select count(*) from population.ods_user_file_list ; 115
        String sink_hudi_hive_user_file_list = "CREATE TABLE sink_user_file_list (\n" +
                "  user_id      bigint   ,\n" +
                "  account      string   ,\n" +
                "  account_name string   ,\n" +
                "  user_file_id string   ,\n" +
                "  fileName     string   ,\n" +
                "  format       string   ,\n" +
                "  size         bigint   ,\n" +
                "  url          string   ,\n" +
                "  remark       string   ,\n" +
                "  add_time     string   ,\n" +
                "  update_time  string   ,\n" +
                "  primary key(user_file_id) not enforced --指定uuid 主键 \n" +
                ")\n" +
                "with(\n" +
                "  'connector'='hudi',\n" +
                "  'path'= 'hdfs://172.17.49.195:9000/user/flink/hudi/population/ods_user_file_list/',\n" +
                "  'table.type'= 'COPY_ON_WRITE', -- 默认COPY_ON_WRITE,可选MERGE_ON_READ\n" +
                "  'hoodie.datasource.write.recordkey.field'= 'user_file_id', -- 主键\n" +
                "  'write.precombine.field'= 'add_time', -- 自动precombine的字段\n" +
                "  'write.operation'= 'upsert',  \n" +
                "  'write.tasks'= '4',\n" +
                "  'write.rate.limit'= '10000', -- 限速\n" +
                "  'compaction.tasks'= '4',\n" +
                "  'compaction.async.enabled'= 'true', -- 是否开启异步压缩\n" +
                "  'compaction.trigger.strategy'= 'num_commits', -- 按次数压缩\n" +
                "  'compaction.delta_commits'= '1', -- 默认为5\n" +
                "  'changelog.enabled'= 'true', -- 开启changelog变更\n" +
                "  'read.streaming.enabled'= 'true', -- 开启流读\n" +
                "  'read.streaming.check-interval'= '60', -- 检查间隔，默认60s\n" +
                "  'hive_sync.enable'='true', -- required，开启hive同步功能\n" +
                "  'hive_sync.mode' = 'hms', -- required, 将hive sync mode设置为hms, 默认jdbc\n" +
                "  'hive_sync.metastore.uris' = 'thrift://172.17.49.195:9083', -- metastore的端口\n" +
                "  'hive_sync.db' = 'population', -- hive新建数据库名\n" +
                "  'hive_sync.table' = 'ods_user_file_list', -- hive新建表名\n" +
                "  'hive_sync.support_timestamp' = 'true' -- 兼容hive timestamp类型\n" +
                ")";



        String picture_list_transform_sql = " insert into sink_user_file_list \n" +
                "  select \n" +
                "      a.userId\n" +
                "     ,c.username\n" + 
                "     ,c.telephone\n" +  
                "     ,a.userFileId\n" +
                "     ,a.fileName\n" +
                "     ,a.extendName\n" +
                "     ,b.fileSize\n" +
                "     ,b.fileUrl\n" +
                "     ,a.filePath\n" +
                "     ,b.createTime\n" +
                "     ,a.uploadTime\n" +
                "  from source_userfile a \n" +
                "  join source_file b on a.fileId = b.fileId\n" +
                "  join source_user c on a.userId = c.userId\n"
                ;
//                "  where  a.extendName in (select fileExtendName from source_fileclassification where fileTypeId=1 ) \n" +
//                "  where  1=1 ";


        //3.执行语句
        tableEnv.executeSql(source_userfile);
        tableEnv.executeSql(source_file);
        tableEnv.executeSql(source_user);
        tableEnv.executeSql(source_fileclassification);
        tableEnv.executeSql(sink_hudi_hive_user_file_list);
        tableEnv.executeSql(picture_list_transform_sql);
    }

}

```

代码说明：

- 设置检查点的存储文件

- 创建输入表、输出表。按照业务需求加工输入表数据后写入输出表

  

#### 2、编译代码

编译alinesno-data-hudi工程

#### 3、部署jar包

将jar包上传到flink的Submit New Job界面，设置运行主类后提交。在Jobs-Running Jobs界面，检查到任务正常运行，即可等待数据变化，捕获数据变更写入数据湖。

a)、查看部署信息

<img :src="$withBase('/operation/data_hudi_17.png')">

b)、查看部署结果

查看hudi表对应的hdfs数据

```shell
[root@hadoopmaster ~]# hadoop fs -ls hdfs://172.17.49.195:9000/user/flink/hudi/population/ods_user_file_list
```

查看映射的hive表数据

```shell
hive> use population;
OK
Time taken: 0.029 seconds
hive> show tables;
OK
ods_user_file_list
Time taken: 0.032 seconds, Fetched: 1 row(s)
hive> select *from ods_user_file_list ;
```
